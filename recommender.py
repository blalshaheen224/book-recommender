import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import MinMaxScaler
import scipy.sparse

df = pd.read_csv("/content/all_book.csv")

# 1. ØªØ­ÙˆÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…
df["Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª"] = pd.to_numeric(df["Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª"], errors='coerce')

# 2. Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ Ù‚ÙŠÙ… ØºÙŠØ± ØµØ§Ù„Ø­Ø©
df.dropna(subset=["Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª"], inplace=True)

# 3. Ø¥Ø¹Ø§Ø¯Ø© Ø¶Ø¨Ø· Ø§Ù„ÙÙ‡Ø±Ø³
df.reset_index(drop=True, inplace=True)

df.dropna(subset=['Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙƒØªØ§Ø¨', 'Ø§Ù„Ù…Ø¤Ù„Ù', 'Ø§Ù„ØªØµÙ†ÙŠÙ', 'Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª', 'Ø§Ù„ÙˆØµÙ'], inplace=True)
df["features"] = df["Ø§Ù„Ù…Ø¤Ù„Ù"] + " " + df["Ø§Ù„ØªØµÙ†ÙŠÙ"] + " " + df["Ø§Ù„ÙˆØµÙ"]
arabic_stopwords = [
    "ÙÙŠ", "Ù…Ù†", "Ø¥Ù„Ù‰", "Ø¹Ù„Ù‰", "Ø£Ù†", "Ø¹Ù†", "Ù…Ø§", "Ù…Ø¹", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡",
    "ÙƒØ§Ù†", "Ù„Ø¯ÙŠ", "Ù„Ù‚Ø¯", "Ø¥Ù†", "Ø°Ù„Ùƒ", "Ø«Ù…", "ÙƒÙ„", "Ù‚Ø¯", "Ù„Ø§", "Ù„Ù…",
    "Ù„Ù‡", "ÙÙŠÙ‡Ø§", "Ù‡Ùˆ", "Ù‡ÙŠ", "Ø£Ùˆ", "Ø£ÙƒØ«Ø±", "Ø¨ÙŠÙ†", "Ø­ØªÙ‰", "Ø¥Ø°Ø§", "Ø¨Ø¹Ø¯"
    # Ù…Ù…ÙƒÙ† ØªØ²ÙˆØ¯ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©
]
vectorizer = TfidfVectorizer(stop_words=arabic_stopwords, max_features=1000)
text_features = vectorizer.fit_transform(df["features"])

scaler = MinMaxScaler()
pages_scaled = scaler.fit_transform(df[["Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª"]])
X = scipy.sparse.hstack([text_features, pages_scaled])
X = X.tocsr()
model = NearestNeighbors(n_neighbors=6, metric='cosine')
model.fit(X)


def recommend_books(title):
    title = title.strip()
    if title not in df['Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙƒØªØ§Ø¨'].values:
        print("this book is not availability please enter this book maunally")
        return
    idx = df[df['Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙƒØªØ§Ø¨'] == title].index[0]

    distances, indices = model.kneighbors(X[idx])
    print(f"\nğŸ“š Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù€ '{title}':")
    for i in range(1, len(indices[0])):
        book_title = df.iloc[indices[0][i]][0]
        print(book_title)


# 9. ØªØ§Ø¨Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙƒØªØ§Ø¨ Ø¬Ø¯ÙŠØ¯
def recommend_new_book(author, category, pages, description):
    new_text = f"{author} {category} {description}"
    new_text_vec = vectorizer.transform([new_text])
    pages_scaled = scaler.transform([[pages]])
    new_vec = scipy.sparse.hstack([new_text_vec, pages_scaled])

    distances, indices = model.kneighbors(new_vec)
    print("\nğŸ“š Ø£Ù‚Ø±Ø¨ Ø§Ù„ÙƒØªØ¨ Ù„Ù‡Ø°Ø§ Ø§Ù„ÙˆØµÙ:")
    for i in range(5):
        print(f"- {df.iloc[indices[0][i]][0]}")


while True:
    print("\nğŸ“˜ Ø£Ø¯Ø®Ù„ Ø§Ø³Ù… ÙƒØªØ§Ø¨ (Ø£Ùˆ Ø§ÙƒØªØ¨ 'Ø¬Ø¯ÙŠØ¯' Ù„Ø¥Ø¶Ø§ÙØ© ÙƒØªØ§Ø¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ - Ø£Ùˆ 'ØªÙˆÙ‚Ù' Ù„Ù„Ø®Ø±ÙˆØ¬):")
    user_input = input(">> ").strip()

    if user_input.lower() == "ØªÙˆÙ‚Ù":
        print("ğŸ‘‹ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬.")
        break

    elif user_input.lower() == "Ø¬Ø¯ÙŠØ¯":
        print("âœï¸ Ø£Ø¯Ø®Ù„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø¬Ø¯ÙŠØ¯:")
        author = input("Ø§Ø³Ù… Ø§Ù„Ù…Ø¤Ù„Ù: ").strip()
        category = input("Ø§Ù„ØªØµÙ†ÙŠÙ: ").strip()

        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª
        while True:
            try:
                pages = int(input("Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: ").strip())
                break
            except ValueError:
                print("âš ï¸ Ù…Ù† ÙØ¶Ù„Ùƒ Ø£Ø¯Ø®Ù„ Ø±Ù‚Ù… ØµØ­ÙŠØ­ Ù„Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª.")

        description = input("Ø§Ù„ÙˆØµÙ: ").strip()
        recommend_new_book(author, category, pages, description)

    else:
        recommend_books(user_input)

